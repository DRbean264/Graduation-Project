{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 人脸关键点批量检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_from_files(files_list,min_size,resize=False):\n",
    "    \"\"\"\n",
    "    从文件读取图像，存放在一个列表中，元素为np.ndarray\n",
    "    params:\n",
    "    files_list:文件路径列表\n",
    "    min_size:读入的图像最短边缩放到多长\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for f in files_list:\n",
    "        image = cv2.imread(f)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if resize == True:\n",
    "            scaled_shape = get_scaled_wh(image.shape[:2],min_size)\n",
    "            image = cv2.resize(image,scaled_shape[::-1],interpolation=cv2.INTER_LINEAR)\n",
    "        images.append(image)\n",
    "    return images\n",
    "\n",
    "def get_frames_through_idx(video_file,start,stop,min_size,resize=False):\n",
    "    \"\"\"\n",
    "    从视频文件读取一定数量的帧，存放在一个列表中，元素为np.ndarray\n",
    "    params:\n",
    "    video_file:视频文件路径列表\n",
    "    start:指标从1开始，开始的帧\n",
    "    stop:结束的帧\n",
    "    min_size:读入的图像最短边缩放到多长\n",
    "    \"\"\"\n",
    "    video_capture = cv2.VideoCapture(video_file)\n",
    "    \n",
    "    frames = []\n",
    "    count = 1\n",
    "    while video_capture.isOpened():\n",
    "        if count == stop + 1:\n",
    "            break\n",
    "        \n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # 开始存储帧\n",
    "        if count >= start:\n",
    "#             frame = cv2.flip(frame, -1)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            if resize == True:\n",
    "                scaled_shape = get_scaled_wh(frame.shape[:2],min_size)\n",
    "                frame = cv2.resize(frame,scaled_shape[::-1],interpolation=cv2.INTER_LINEAR)\n",
    "            frames.append(frame)\n",
    "        \n",
    "        count += 1\n",
    "    video_capture.release()\n",
    "    \n",
    "    return frames\n",
    "        \n",
    "def yield_frames_through_idx(video_file,batch_size,min_size,resize=False):\n",
    "    \"\"\"\n",
    "    从视频文件读取一定数量的帧，存放在一个列表中，元素为np.ndarray\n",
    "    \n",
    "    params:\n",
    "    video_file:视频文件路径列表\n",
    "    start:指标从1开始，开始的帧\n",
    "    stop:结束的帧\n",
    "    min_size:读入的图像最短边缩放到多长\n",
    "    \"\"\"\n",
    "    video_capture = cv2.VideoCapture(video_file)\n",
    "\n",
    "    while True:\n",
    "        frames = []\n",
    "        while video_capture.isOpened():\n",
    "            for b in range(batch_size):\n",
    "                ret, frame = video_capture.read()\n",
    "                #  读到视频结尾的话，退出循环\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                if resize == True:\n",
    "                    scaled_shape = get_scaled_wh(frame.shape[:2],min_size)\n",
    "                    frame = cv2.resize(frame,scaled_shape[::-1],interpolation=cv2.INTER_LINEAR)\n",
    "                frames.append(frame)\n",
    "            break\n",
    "        if len(frames) != batch_size:\n",
    "            video_capture.release()\n",
    "    \n",
    "        yield frames\n",
    "\n",
    "def get_scaled_wh(current_size,min_size):\n",
    "    \"\"\"\n",
    "    计算缩放后的宽高，计算方法为把短边缩放到指定的边长，等比缩放长边\n",
    "    params:\n",
    "    current_size:(h,w)\n",
    "    min_size:最短的边长为多长\n",
    "    \n",
    "    return:\n",
    "    等比例缩放后的高宽(h,w)\n",
    "    \"\"\"\n",
    "    h_c,w_c = current_size\n",
    "    \n",
    "    scale_ratio = max(min_size/w_c,min_size/h_c)\n",
    "    if h_c > w_c:\n",
    "        h_s = int(h_c * scale_ratio)\n",
    "        w_s = min_size\n",
    "    else:\n",
    "        h_s = min_size\n",
    "        w_s = int(w_c * scale_ratio)\n",
    "    return h_s,w_s\n",
    "\n",
    "def get_location_landmark(images,number_of_times_to_upsample=0):\n",
    "    \"\"\"\n",
    "    利用face_recognition库高速率实时的获取图像中人脸位置以及关键点位置，调用GPU运算\n",
    "    params:\n",
    "    images：一个列表，每个元素为一张图片的ndarray\n",
    "    number_of_times_to_upsample：图像上采样次数，为了高效，取0\n",
    "    valid：这批图片的有效性，如果存在没有检测到人脸的帧，则设置为False\n",
    "    \"\"\"\n",
    "    # return a list of tuples of found face locations in css (top, right, bottom, left) order\n",
    "    # [[(72, 160, 140, 92), (24, 101, 71, 54), (17, 296, 57, 257), (48, 226, 95, 179), (1, 188, 41, 149), (1, 140, 41, 101)]]\n",
    "    t0 = time.clock()\n",
    "    batch_of_face_locations = face_recognition.batch_face_locations(images, number_of_times_to_upsample)\n",
    "    t1 = time.clock()\n",
    "#     print(\"Batch face locations and landmarks running time on {} image(s): {:.4f} s (Equivalent {:.2f} fps)\" \\\n",
    "#           .format(len(images),t1-t0,len(images)/(t1-t0)))\n",
    "    \n",
    "    valid = True\n",
    "    batch_of_landmarks = []\n",
    "    for idx,face_locations in enumerate(batch_of_face_locations):\n",
    "        # 在这个应用中找到一个人脸说明正确\n",
    "        if len(face_locations) == 1:\n",
    "            # [{},{},{},...,{}]\n",
    "            landmarks = face_recognition.face_landmarks(images[idx], face_locations=face_locations, model='large')\n",
    "            batch_of_landmarks.append(landmarks)\n",
    "        # 可能在这帧中找到多余的人脸，剔除小的人脸，这里假设最大的框是正确的\n",
    "        elif len(face_locations) >= 2:\n",
    "            idx_true = 0\n",
    "            max_area = 0\n",
    "            for id_face,face in enumerate(face_locations):\n",
    "                top,right,bottom,left = face\n",
    "                area = (bottom - top) * (right - left)\n",
    "                if area > max_area:\n",
    "                    max_area = area\n",
    "                    idx_true = id_face\n",
    "            landmarks = face_recognition.face_landmarks(images[idx], face_locations=face_locations[idx_true:idx_true+1],model='large')\n",
    "            batch_of_landmarks.append(landmarks)\n",
    "        # 也可能没找到人脸\n",
    "        else:\n",
    "            print(\"No faces are found in this image!!!\")\n",
    "            valid = False\n",
    "            batch_of_landmarks.append([])\n",
    "        \n",
    "    return batch_of_face_locations,batch_of_landmarks,valid\n",
    "\n",
    "# ========================\n",
    "#   Test Benchmark\n",
    "# ========================\n",
    "# image = cv2.imread(\"./Pics/Cow.jpg\")\n",
    "# scaled_shape = get_scaled_wh(image.shape[:2],480)\n",
    "# image = cv2.resize(image,scaled_shape[::-1],interpolation=cv2.INTER_LINEAR)\n",
    "# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# a,b = get_location_landmark([image,image])\n",
    "# print(a)\n",
    "# print(b)\n",
    "\n",
    "def draw_location_landmark(images,draw_num=1,draw_location=True,batch_of_face_locations=None,draw_landmarks=True,batch_of_landmarks=None):\n",
    "    \"\"\"\n",
    "      可视化检测框和关键点\n",
    "      \n",
    "      params:\n",
    "      images:图像列表\n",
    "      draw_num:限制处理的图像数\n",
    "      draw_location:Boolean，是否画检测框\n",
    "      draw_landmarks:Boolean，是否画关键点\n",
    "      batch_of_face_locations:face_recognition得到的检测框位置的列表\n",
    "      batch_of_landmarks:face_recognition得到的人脸关键点位置的列表\n",
    "    \"\"\"\n",
    "    for idx,image in enumerate(images):\n",
    "        if idx == draw_num:\n",
    "            break\n",
    "        \n",
    "        # 画矩形框\n",
    "        if draw_location:\n",
    "            face_locations = batch_of_face_locations[idx]        \n",
    "            for face_location in face_locations:\n",
    "                # Print the location of each face in this frame\n",
    "                top, right, bottom, left = face_location\n",
    "                cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        # 画关键点\n",
    "        if draw_landmarks:\n",
    "            landmarks = batch_of_landmarks[idx]  #  dictionary or 29*2 array\n",
    "            try:\n",
    "                for landmark in landmarks:\n",
    "                    for _,points_list in landmark.items():\n",
    "                        for point in points_list:\n",
    "                            cv2.circle(image, point, radius=4, color=(0, 255, 0), thickness=-1)\n",
    "            except:\n",
    "                for landmark in landmarks:\n",
    "                    cv2.circle(image, tuple(landmark), radius=4, color=(0, 255, 0), thickness=-1)\n",
    "                        \n",
    "        cv2.namedWindow('Face Recognition Display {}'.format(idx))\n",
    "        cv2.imshow('Face Recognition Display {}'.format(idx), image[...,::-1])\n",
    "        \n",
    "    if cv2.waitKey(0):\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "# ========================\n",
    "#   Test Benchmark\n",
    "# ========================\n",
    "# image = cv2.imread(\"./Pics/Family.jpg\")\n",
    "# scaled_shape = get_scaled_wh(image.shape[:2],480)\n",
    "# image = cv2.resize(image,scaled_shape[::-1],interpolation=cv2.INTER_LINEAR)\n",
    "# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# batch_of_face_locations,batch_of_landmarks = get_location_landmark([image,image])\n",
    "# draw_location_landmark([image],1,True,batch_of_face_locations,True,batch_of_landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取15帧，并计算检测框和人脸关键点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "(960, 544, 3)\n"
     ]
    }
   ],
   "source": [
    "# 改成自己本地电脑上的视频路径\n",
    "video_file = './Test_Videos/stable1.mp4'\n",
    "batch_size = 15\n",
    "\n",
    "# 自然指标，从1开始\n",
    "frames = get_frames_through_idx(video_file,1,15,800,resize=False)\n",
    "print(len(frames))\n",
    "print(frames[0].shape)\n",
    "\n",
    "batch_of_face_locations,batch_of_landmarks,_ = get_location_landmark(frames,0)\n",
    "\n",
    "num_chin = len(batch_of_landmarks[0][0]['chin'])\n",
    "num_eye = len(batch_of_landmarks[0][0]['left_eye'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_show = np.array(frames).copy()\n",
    "draw_location_landmark(frames_show,15,False,batch_of_face_locations,True,batch_of_landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键点滤波"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmarks_filter(batch_of_landmarks,span=1):\n",
    "    \"\"\"\n",
    "    提取脸颊，左右眼的关键点，并做高斯滤波，消除抖动\n",
    "\n",
    "    params:\n",
    "    batch_of_landmarks:dlib算法提取出的人脸关键点，[[{}],[{}],...]\n",
    "    span:滤波的跨度，越小越平滑\n",
    "\n",
    "    return:\n",
    "    batch_of_filtered_landmarks:shape:(batch, 29, 2)\n",
    "    \"\"\"\n",
    "    # 提取感兴趣的关键点\n",
    "    batch_of_extracted_landmarks = []\n",
    "    for landmarks in batch_of_landmarks:\n",
    "        flatten_array = []\n",
    "        for landmark in landmarks:\n",
    "            for k,v in landmark.items():\n",
    "                if k in ['chin','left_eye','right_eye']:\n",
    "                    flatten_array.append(v)\n",
    "        batch_of_extracted_landmarks.append(np.concatenate(flatten_array,axis=0))\n",
    "    batch_of_extracted_landmarks = np.array(batch_of_extracted_landmarks).reshape(len(batch_of_landmarks),-1)  #shape:batch*58\n",
    "    \n",
    "    # 高斯滤波\n",
    "    df_landmarks = pd.DataFrame(batch_of_extracted_landmarks)\n",
    "    df_landmarks = df_landmarks.ewm(span,adjust=False).mean()\n",
    "    \n",
    "    batch_of_filtered_landmarks = np.rint(df_landmarks.to_numpy()).astype(np.int32).reshape(-1,29,2)\n",
    "    return batch_of_filtered_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 29, 2)\n",
      "(15, 960, 544, 3)\n"
     ]
    }
   ],
   "source": [
    "batch_of_filtered_landmarks = landmarks_filter(batch_of_landmarks,span=10)\n",
    "print(batch_of_filtered_landmarks.shape)\n",
    "\n",
    "batch_of_frames = np.array(frames)\n",
    "print(batch_of_frames.shape)\n",
    "draw_location_landmark(batch_of_frames.copy(),3,False,batch_of_face_locations,True,batch_of_filtered_landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 旋转人脸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_face(frames, batch_of_filtered_landmarks):\n",
    "    \"\"\" \n",
    "    以两眼中心旋转帧\n",
    "    params:\n",
    "    frames:包含人脸的帧\n",
    "    batch_of_filtered_landmarks:经过滤波之后的关键点\n",
    "    \n",
    "    return:\n",
    "    rotated_frames:旋转后的帧图像，与原图大小一致\n",
    "    eye_centers:每帧中两眼连线中心点\n",
    "    angles:每帧旋转的角度\n",
    "    \"\"\"\n",
    "    # 转化为Tensor，加速运算\n",
    "    batch_of_filtered_landmarks = tf.convert_to_tensor(batch_of_filtered_landmarks,dtype=tf.float32)\n",
    "    # 提取左右眼数组,shape:batch,6,2\n",
    "    left_eyes = batch_of_filtered_landmarks[:,num_chin:(num_chin+num_eye),:]\n",
    "    right_eyes = batch_of_filtered_landmarks[:,(num_chin+num_eye):,:]\n",
    "    # 计算左右眼中心,shape:batch,2\n",
    "    left_eye_centers = K.mean(left_eyes, axis=1)\n",
    "    right_eye_centers = K.mean(right_eyes, axis=1)\n",
    "    # 计算角度angle:shape=batch,1\n",
    "    dxy = right_eye_centers - left_eye_centers\n",
    "    angles = tf.atan2(dxy[...,1], dxy[...,0]) * 180. / np.pi\n",
    "    # 计算眼睛连线中点,shape:batch,2\n",
    "    eye_centers = tf.add(left_eye_centers,right_eye_centers) / 2.\n",
    "    \n",
    "    angles = angles.numpy()\n",
    "    eye_centers = eye_centers.numpy()\n",
    "    \n",
    "    # 以眼睛连线中点为旋转中心逆时针旋转图片angle角度\n",
    "    rotated_frames = []\n",
    "    for frame,eye_center,angle in zip(frames,eye_centers,angles):\n",
    "        rotate_matrix = cv2.getRotationMatrix2D(tuple(eye_center), angle, scale=1)\n",
    "        rotated_frame = cv2.warpAffine(frame, rotate_matrix, (frame.shape[1], frame.shape[0]))\n",
    "        \n",
    "        rotated_frames.append(rotated_frame)\n",
    "    rotated_frames = np.array(rotated_frames)\n",
    "    \n",
    "    return rotated_frames, eye_centers, angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 960, 544, 3)\n"
     ]
    }
   ],
   "source": [
    "rotated_frames, eye_centers, angles = rotate_face(batch_of_frames, batch_of_filtered_landmarks)\n",
    "\n",
    "print(rotated_frames.shape)\n",
    "draw_location_landmark(rotated_frames.copy(),3,False,batch_of_face_locations,False,batch_of_filtered_landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 旋转关键点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_landmarks(eye_centers, batch_of_filtered_landmarks, angles):\n",
    "    \"\"\" \n",
    "    旋转关键点\n",
    "    params:\n",
    "    eye_centers:每帧中两眼连线中心点\n",
    "    batch_of_filtered_landmarks:需要旋转的关键点坐标\n",
    "    angles:每帧旋转的角度\n",
    "    \n",
    "    return:\n",
    "    rotated_landmarks:旋转后的关键点坐标\n",
    "    \"\"\"\n",
    "    batch_of_filtered_landmarks = tf.convert_to_tensor(batch_of_filtered_landmarks,dtype=tf.float32)\n",
    "    eye_centers = tf.convert_to_tensor(eye_centers.reshape(len(batch_of_filtered_landmarks),1,-1),dtype=tf.float32)\n",
    "    angles = tf.convert_to_tensor(angles.reshape(len(batch_of_filtered_landmarks),-1) * np.pi / 180.,dtype=tf.float32)\n",
    "    \n",
    "    batch_of_filtered_landmarks_conv = batch_of_filtered_landmarks - eye_centers  #shape:batch,29,2\n",
    "    \n",
    "    rotate_mat = tf.concat([tf.cos(angles),tf.sin(angles),-tf.sin(angles),tf.cos(angles)],axis=-1)  #shape:batch,4\n",
    "    rotate_mat = tf.reshape(rotate_mat,shape=(-1,2,2))  #shape:batch,2,2\n",
    "    \n",
    "    rotated_landmarks = tf.transpose(tf.matmul(rotate_mat,tf.transpose(batch_of_filtered_landmarks_conv,perm=[0,2,1])),perm=[0,2,1])\\\n",
    "    + eye_centers  #shape:batch,29,2\n",
    "    \n",
    "    return rotated_landmarks.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_landmarks = rotate_landmarks(eye_centers, batch_of_filtered_landmarks, angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 29, 2)\n"
     ]
    }
   ],
   "source": [
    "print(rotated_landmarks.shape)\n",
    "draw_location_landmark(rotated_frames.copy(),3,False,batch_of_face_locations,True,rotated_landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 切割出人脸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_face(frames, eye_centers, rotated_landmarks):\n",
    "    \"\"\" \n",
    "    根据关键点切割出人脸\n",
    "    params:\n",
    "    image_array: numpy array of a single image\n",
    "    size: single int value, size for w and h after crop\n",
    "    landmarks: dict of landmarks for facial parts as keys and tuple of coordinates as values\n",
    "    \n",
    "    return:\n",
    "    cropped_img: numpy array of cropped image\n",
    "    left, top: left and top coordinates of cropping\n",
    "    \"\"\"\n",
    "    lefts = np.rint(np.min(rotated_landmarks[...,0],axis=1))  #shape:batch,\n",
    "    rights = np.rint(np.max(rotated_landmarks[...,0],axis=1))  #shape:batch,\n",
    "    \n",
    "    widths = rights - lefts  #shape:batch,\n",
    "    width = np.rint(np.mean(widths))  #scalar\n",
    "    rights = lefts + width  #shape:batch,\n",
    "    \n",
    "    heights = np.max(rotated_landmarks[...,1],axis=1) - eye_centers[...,1]  #shape:batch,\n",
    "    height = np.rint(np.mean(heights))  #scalar,这个height并不是最终切割人脸的高度，而是两眼中心到脸颊底部的距离,实际高度乘以1.2倍\n",
    "    \n",
    "    bottoms = np.max(rotated_landmarks[...,1],axis=1)  #shape:batch,\n",
    "    bottoms = np.rint(bottoms)\n",
    "    tops = np.rint(bottoms - height * 1.2)\n",
    "    \n",
    "    cropped_frames = []\n",
    "    for left, top, right, bottom, frame in zip(lefts, tops, rights, bottoms, frames):\n",
    "        pil_frame = Image.fromarray(frame)\n",
    "        \n",
    "        cropped_frame = pil_frame.crop((left, top, right, bottom))\n",
    "        cropped_frames.append(np.array(cropped_frame))\n",
    "    cropped_frames = np.array(cropped_frames)  \n",
    "        \n",
    "    return cropped_frames,lefts,tops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_frames,lefts,tops = crop_face(rotated_frames, eye_centers, rotated_landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 300, 275, 3)\n",
      "(15,)\n",
      "(15,)\n"
     ]
    }
   ],
   "source": [
    "print(cropped_frames.shape)\n",
    "print(lefts.shape)\n",
    "print(tops.shape)\n",
    "draw_location_landmark(cropped_frames.copy(),3,False,batch_of_face_locations,False,rotated_landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键点重新校准"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_landmark(landmarks, lefts, tops):\n",
    "    \"\"\"\n",
    "    需要重新计算关键点的位置以匹配剪裁后的图像\n",
    "    params:\n",
    "    landmarks:需要变换坐标的关键点\n",
    "    lefts:每帧左侧剪裁的距离\n",
    "    tops:每帧上侧剪裁的距离\n",
    "    \n",
    "    return: \n",
    "    transferred_landmarks:转换后的关键点坐标\n",
    "    \"\"\"\n",
    "    batch_size = len(landmarks)\n",
    "    shift = np.concatenate([lefts.reshape(batch_size,1,-1),tops.reshape(batch_size,1,-1)],axis=-1)  #shape:batch,1,2\n",
    "    transferred_landmarks = landmarks - shift\n",
    "    \n",
    "    return transferred_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "transferred_landmarks = transfer_landmark(rotated_landmarks, lefts, tops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 29, 2)\n"
     ]
    }
   ],
   "source": [
    "print(transferred_landmarks.shape)\n",
    "draw_location_landmark(cropped_frames.copy(),3,False,batch_of_face_locations,True,transferred_landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 肤色分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于HSV的肤色分割，比较偏经验，通用性不强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skin_HSV(image):\n",
    "    # Taking a copy of the image\n",
    "    img = image.copy()\n",
    "    # Converting from BGR Colours Space to HSV\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    # Defining HSV Threadholds\n",
    "    lower_threshold = np.array([0, 60, 50], dtype=np.uint8)\n",
    "    upper_threshold = np.array([255, 255, 255], dtype=np.uint8)\n",
    "\n",
    "    # Single Channel mask,denoting presence of colours in the about threshold\n",
    "    skinMask = cv2.inRange(img, lower_threshold, upper_threshold)\n",
    "\n",
    "    # Cleaning up mask using Gaussian Filter\n",
    "    skinMask = cv2.GaussianBlur(skinMask, (3, 3), 0)\n",
    "\n",
    "    # Extracting skin from the threshold mask\n",
    "    skin = cv2.bitwise_and(img, img, mask=skinMask)\n",
    "\n",
    "    # Return the Skin image\n",
    "    return cv2.cvtColor(skin, cv2.COLOR_HSV2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于YUV空间的阈值分割，通用性强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_Skin_YCrCb_Otsu(frames):\n",
    "    \"\"\"\n",
    "      大津算法\n",
    "    \"\"\"\n",
    "    skins = []\n",
    "    masks = []\n",
    "    for frame in frames:\n",
    "        \n",
    "        # 转至YCrCb颜色空间\n",
    "        ycrcb_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2YCrCb)\n",
    "        cr = ycrcb_frame[:, :, 1]\n",
    "\n",
    "        # 通过OTSU算法从Cr通道提取脸部前景区域\n",
    "        _, mask = cv2.threshold(cr, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "\n",
    "        # 腐蚀膨胀去除噪声\n",
    "        kernel_size = min(frame.shape[0], frame.shape[1]) // 40\n",
    "        #一个ndarray，一个类似卷积核的东西，用于对图像做腐蚀和膨胀操作\n",
    "        element = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_size, kernel_size))\n",
    "        mask = cv2.erode(mask, element)\n",
    "        mask = cv2.dilate(mask, element)\n",
    "\n",
    "        # 保留最大轮廓\n",
    "        # contours为一个点集列表，每个元素为一个ndarray，例如：(204, 1, 2)，代表204个点组成的边界线\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        max_index = 0\n",
    "        max_val = -1\n",
    "        for idx, c in enumerate(contours):\n",
    "            if c.shape[0] > max_val:\n",
    "                max_val = c.shape[0]\n",
    "                max_index = idx\n",
    "        canvas = mask * 0\n",
    "        mask = cv2.drawContours(canvas, contours, max_index, 1, -1)\n",
    "        \n",
    "        mask = np.expand_dims(mask, axis=2)\n",
    "        skins.append(mask * frame)\n",
    "        masks.append(mask)\n",
    "        \n",
    "    return np.array(skins),np.array(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 300, 275, 3)\n",
      "uint8\n",
      "(15, 300, 275, 1)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "skin_frames,skin_masks = extract_Skin_YCrCb_Otsu(cropped_frames)\n",
    "print(skin_frames.shape)\n",
    "print(skin_frames.dtype)\n",
    "print(skin_masks.shape)\n",
    "print(skin_masks.dtype)\n",
    "\n",
    "draw_location_landmark(skin_frames.copy(),3,False,batch_of_face_locations,False,transferred_landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可以转换为不同颜色空间/通道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RGB2YUV(frames):\n",
    "    kernel = tf.constant([[0.229,0.587,0.114],[-0.169,-0.331,0.5],[0.5,-0.419,-0.081]],dtype=tf.float32)\n",
    "    bias = tf.constant([0,128,128],dtype=tf.float32,shape=(3,))\n",
    "    \n",
    "    frames = tf.convert_to_tensor(frames,tf.float32)\n",
    "    frames = tf.transpose(frames,perm=[0,1,3,2])\n",
    "    frames = tf.matmul(kernel,frames)\n",
    "    frames = tf.transpose(frames,perm=[0,1,3,2]) + bias\n",
    "    \n",
    "    return frames.numpy().astype(np.uint8)\n",
    "\n",
    "def RGB2G(frames,channel=1):\n",
    "    g_channel = frames[...,1:2]\n",
    "    r_channel = np.zeros(shape=g_channel.shape,dtype=frames.dtype)\n",
    "    b_channel = np.zeros(shape=g_channel.shape,dtype=frames.dtype)\n",
    "    \n",
    "    if channel == 3:\n",
    "        return np.concatenate([r_channel,g_channel,b_channel],axis=-1)  # shape:batch*w*h*3\n",
    "    else:\n",
    "        return frames[...,1:2]  # shape:batch*w*h*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 300, 275, 3)\n"
     ]
    }
   ],
   "source": [
    "skin_frames_YUV = RGB2YUV(skin_frames.copy())\n",
    "print(skin_frames_YUV.shape)\n",
    "draw_location_landmark(skin_frames_YUV.copy(),3,False,batch_of_face_locations,False,transferred_landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 300, 275, 1)\n"
     ]
    }
   ],
   "source": [
    "skin_frames_Green = RGB2G(skin_frames)\n",
    "print(skin_frames_Green.shape)\n",
    "draw_location_landmark(skin_frames_Green.copy(),3,False,batch_of_face_locations,False,transferred_landmarks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
